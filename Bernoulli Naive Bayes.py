# -*- coding: utf-8 -*-
"""Assignment 2 - Final Code - Team 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OgxRqkkrVGsP3kI2P_mEXFNChYBsgRWg

# Import necessary libraries
"""

import pandas as pd
import numpy as np
import time

from sklearn.feature_extraction import text 
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import cross_val_score, KFold
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn import svm
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn import metrics 

import nltk
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer
from nltk import word_tokenize
from nltk.corpus import wordnet
from sklearn.ensemble import RandomForestClassifier
from nltk.tokenize import RegexpTokenizer

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

"""# Functions

## Naive Bayes Classifier
"""

class NaiveBayes(object):
  '''
  Naive Bayes classifier model.
  '''

  def fit(self, X, y):
    '''
    Function to train the Naive Bayes model.

    Parameters
    ------------
    X: {array-like}, shape = [n_examples, n_features]
        Training vectors, where n_examples is the number of examples and n_features is the number of features.
    y : array-like, shape = [n_examples]
        Target values.

    Returns
    ------------
    self : object
    '''
    # class_labels = np.unique(y)
    class_labels, y_counts = np.unique(y, return_counts=True)
    sum_counts = y_counts.sum()
    self.theta_ks = y_counts/sum_counts
    self.theta_j_ks = []
    for class_label in class_labels:
      length = sum(y==class_label)
      # Calculate theta_j_k with Laplace Smoothing
      theta_j_k = (X[y==class_label].sum(axis=0) + 1) / (length + 2)
      self.theta_j_ks.append(theta_j_k)
    self.theta_j_ks = np.array(self.theta_j_ks)

    return self

  def predict(self, X):
    '''
    Predict the class.

    Parameters
    ------------
    X: {array-like}, shape = [n_examples, n_features]
        Training vectors, where n_examples is the number of examples and n_features is the number of features.

    Returns
    ------------
    y_pred: {array-like}, shape = [n_examples]
        Predicted classes.
    '''
    y_pred = np.argmax(np.log(self.theta_ks) + X.dot(np.log(self.theta_j_ks).T) + (1-X).dot(np.log(1-self.theta_j_ks).T), axis=1)
    return y_pred


  def score(self, X, y):
    '''
    Evaluate the accuracy of the model.

    Parameters
    ------------
    X: {array-like}, shape = [n_examples, n_features]
        Input vectors, where n_examples is the number of examples and n_features is the number of features.
    y : array-like, shape = [n_examples]
        True labels.

    Returns
    ------------
    accuracy: float
        Accuracy of the model.
    '''
    y_pred = self.predict(X)
    num_correct_labels = (y == y_pred).sum()
    accuracy = num_correct_labels / len(y)
    return accuracy

"""## KFold Cross Validation"""

class KFoldCV (object):
    '''
    K-fold cross validation.

    Parameters
    ------------
    estimator: estimator object implementing "fit"
        The machine learning model used for evaluation.
    scaler: {standard scaler object, None} (default=None)
        The standard scaler model used for standardize the features.
    k: int (default=10)
        Number of splits.
    shuffle: boolean (defaul=True)
        Whether or not shuffle the dataset.
    random_state: {int, None} (default=None)
        Random state for the random numbers.
    verbose: {0, 1} (default=0)
        0: no information during training will be printed; 1: accuracy will be printed after each CV 

    Attributes
    -----------
    accuracy_scores: list of float
        List of scores of the estimator for each run of the cross validation.
    '''
    def __init__(self, estimator, scaler=None, k=10, shuffle=True, random_state=None, verbose=0):
        self.estimator = estimator
        self.scaler = scaler
        self.k = k
        self.shuffle = shuffle
        self.random_state = random_state
        self.verbose = verbose
    
    def fit(self, X, y):
        '''
        Function to fit the data into the estimator.

        Parameters
        ------------
        X: {array-like}, shape = [n_examples, n_features]
            Training vectors, where n_examples is the number of examples and n_features is the number of features.
        y : array-like, shape = [n_examples]
            Target values.

        Returns
        ------------
        self : object
        '''
        np.random.seed(seed=self.random_state)
        if self.shuffle:
          indices = np.random.permutation(X.shape[0])
          X = X[indices]
          y = y[indices]
        split = X.shape[0] // 10
        X_split = []
        y_split = []
        for i in range(self.k):
          if i == 0:
            X_split.append(X[:split])
            y_split.append(y[:split])
          elif i == self.k-1:
            X_split.append(X[i*split:])
            y_split.append(y[i*split:])
          else:
            X_split.append(X[i*split:(i+1)*split])
            y_split.append(y[i*split:(i+1)*split])
        self.accuracy_scores = []
        for i in range(self.k):
          X_train = np.array([X_split[j] for j in range(self.k) if j != i])
          y_train = np.array([y_split[j] for j in range(self.k) if j != i])
          X_valid = X_split[i]
          y_valid = y_split[i]
          X_train = np.concatenate(X_train, axis=0)
          y_train = np.concatenate(y_train, axis=0)
          if self.scaler != None:
            self.scaler.fit(X_train)
            X_train = self.scaler.transform(X_train)
            X_valid = self.scaler.transform(X_valid)
          self.estimator.fit(X_train, y_train)
          accuracy = self.estimator.score(X_valid, y_valid)
          self.accuracy_scores.append(accuracy)
          if self.verbose == 1:
            print("CV: {} -> Accuracy: {:.2f}".format(i+1, accuracy))
        return self

"""## Stem and Lemmatization"""

def tokenize(sentence):
  token_list = word_tokenize(sentence)
  filter = '!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'
  for token in token_list:
    if token in filter:
      token_list.remove(token)
  return token_list
def get_wordnet_pos(word):
  """Map POS tag to first character lemmatize() accepts"""
  tag = nltk.pos_tag([word])[0][1][0].upper()
  tag_dict = {"J": wordnet.ADJ,
              "N": wordnet.NOUN,
              "V": wordnet.VERB,
              "R": wordnet.ADV}
  return tag_dict.get(tag, wordnet.NOUN) # default type NOUN
def lemmatization(token_list):
  lemmatized_list = []
  for word in token_list:
    lemmatized_list.append(lemmatizer.lemmatize(word, pos=get_wordnet_pos(word)))
  return lemmatized_list
def stem(token_list):
  stemmed_list = []
  for word in token_list:
    stemmed_list.append(stemmer.stem(word))
  return stemmed_list 
stemmer = SnowballStemmer('english')
lemmatizer = WordNetLemmatizer()

"""# Test Naive Bayes Model Performance"""

train_set = pd.read_csv("train.csv")
test_set = pd.read_csv("test.csv")

"""## Basic Case"""

encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))
body = train_set.body

stop_words = text.ENGLISH_STOP_WORDS
ngram_range_list = [(1, 1), (1, 2)]
max_features_list = [5000, 7000, 10000, 13000]
max_df_list = [0.1, 0.3, 0.5, 0.7, 1.0]
stop_words_list = [None, stop_words]
for stop_words in stop_words_list:
  for ngram_range in ngram_range_list:
    for max_features in max_features_list:
      for max_df in max_df_list:
        vectorizer = CountVectorizer(max_features=max_features, ngram_range=ngram_range, max_df=max_df, stop_words = stop_words, binary=True)
        X = vectorizer.fit_transform(body).toarray()
        model = NaiveBayes()
        my_cv = KFoldCV(model, random_state=1)
        my_cv.fit(X, y)
        scores = my_cv.accuracy_scores
        print("stop_words:{}, ngram_range:{}, max_features: {}; max_df:{} -> average CV score: {}".format(stop_words!=None, ngram_range, max_features, max_df, sum(scores)/len(scores)))

"""## Stem"""

encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))
start = time.time()
body = train_set.body.apply(tokenize).apply(stem).str.join(' ')
end = time.time()
print("running time for stem: {}".format(end - start))

stop_words = text.ENGLISH_STOP_WORDS
ngram_range_list = [(1, 1), (1, 2)]
max_features_list = [5000, 7000, 10000, 13000]
max_df_list = [0.1, 0.3, 0.5, 0.7, 1.0]
stop_words_list = [None, stop_words]
for stop_words in stop_words_list:
  for ngram_range in ngram_range_list:
    for max_features in max_features_list:
      for max_df in max_df_list:
        vectorizer = CountVectorizer(max_features=max_features, ngram_range=ngram_range, max_df=max_df, stop_words = stop_words, binary=True)
        X = vectorizer.fit_transform(body).toarray()
        model = NaiveBayes()
        my_cv = KFoldCV(model, random_state=1)
        my_cv.fit(X, y)
        scores = my_cv.accuracy_scores
        print("stop_words:{}, ngram_range:{}, max_features: {}; max_df:{} -> average CV score: {}".format(stop_words!=None, ngram_range, max_features, max_df, sum(scores)/len(scores)))

"""## Lemmatization"""

encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))
start = time.time()
body = train_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
end = time.time()
print("running time for lemmatization: {}".format(end - start))

stop_words = text.ENGLISH_STOP_WORDS
ngram_range_list = [(1, 1), (1, 2)]
max_features_list = [5000, 7000, 10000, 13000]
max_df_list = [0.1, 0.3, 0.5, 0.7, 1.0]
stop_words_list = [None, stop_words]
for stop_words in stop_words_list:
  for ngram_range in ngram_range_list:
    for max_features in max_features_list:
      for max_df in max_df_list:
        vectorizer = CountVectorizer(max_features=max_features, ngram_range=ngram_range, max_df=max_df, stop_words = stop_words, binary=True)
        X = vectorizer.fit_transform(body).toarray()
        model = NaiveBayes()
        my_cv = KFoldCV(model, random_state=1)
        my_cv.fit(X, y)
        scores = my_cv.accuracy_scores
        print("stop_words:{}, ngram_range:{}, max_features: {}; max_df:{} -> average CV score: {}".format(stop_words!=None, ngram_range, max_features, max_df, sum(scores)/len(scores)))

"""## Best model
Lemmatization, CountVectorizer with stopwords, max_features = 13000, and max_ef=0.5
"""

start = time.time()
model = NaiveBayes()
my_cv = KFoldCV(model, random_state=1)
vectorizer = CountVectorizer(max_features=13000, max_df=0.5, stop_words = text.ENGLISH_STOP_WORDS, binary=True)
X = vectorizer.fit_transform(body).toarray()
my_cv.fit(X, y)
end = time.time()
print("Running time for Naive Bayes: {}".format(end - start))

start = time.time()
model = NaiveBayes()
vectorizer = CountVectorizer(max_features=13000, max_df=0.5, stop_words = text.ENGLISH_STOP_WORDS, binary=True)
X = vectorizer.fit_transform(body).toarray()
model.fit(X, y)
end = time.time()
print("Running time for Naive Bayes: {}".format(end - start))

model = NaiveBayes()
model.fit(X, y)
model.score(X, y)

"""# Logistic Regression

## Basic Case - no stem or lemmatization
"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")
body = train_set.body
encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))

"""### 1-gram"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer()), ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, n_jobs=-1))])
param_grid = {'vect__stop_words': [None, stop_words], 'vect__max_features': [5000, 10000, 15000, None], 'vect__max_df':[0.5, 0.7, 1.0], 
              'lr__C':[0.1, 1, 10], }
gs_lr = GridSearchCV(pipeline_lr, param_grid, n_jobs=-1, cv=10, verbose=10)
gs_lr.fit(body, y)

print(gs_lr.best_params_)
print(gs_lr.best_score_)
gs_lr_best = gs_lr.best_estimator_

"""### 2-grams"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 2))), ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, solver='saga', n_jobs=-1))])
param_grid = {'vect__max_features': [50000, 60000, 70000, None],'vect__max_df':[0.5, 0.7, 1.0], 
              'lr__C':[10, 20]}
gs_lr = GridSearchCV(pipeline_lr, param_grid, n_jobs=-1, cv=10, verbose=10)
gs_lr.fit(body, y)

print(gs_lr.best_params_)
print(gs_lr.best_score_)
gs_lr_best = gs_lr.best_estimator_

pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, max_features=60000, max_df=0.5, ngram_range=(1, 2))),
                        ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, C=20, solver='saga', n_jobs=-1))])
scores = cross_val_score(pipeline_lr, body, y, cv=10)
scores.mean()

"""### 3-grams"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 3))), ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, solver='saga', n_jobs=-1))])
param_grid = {'vect__max_features': [80000, 90000, 100000, None],'vect__max_df':[0.5, 0.7, 1.0], 'vect__min_df':[1, 5, 10],
              'lr__C':[10, 30]}
gs_lr = GridSearchCV(pipeline_lr, param_grid, n_jobs=-1, cv=10, verbose=10)
gs_lr.fit(body, y)

print(gs_lr.best_params_)
print(gs_lr.best_score_)
gs_lr_best = gs_lr.best_estimator_

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words,max_df = 0.7, max_features=80000, ngram_range=(1, 3))), ('norm', Normalizer()),
                        ('lr', LogisticRegression(max_iter=10000, C=30, solver='saga', n_jobs=-1))])
scores = cross_val_score(pipeline_lr, body, y, cv=10)
scores.mean()

"""## Stem"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")
# Stem
body = train_set.body.apply(tokenize).apply(stem).str.join(' ')
body_test = test_set.body.apply(tokenize).apply(stem).str.join(' ')
encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))

"""### 1-gram"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer()), ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, n_jobs=-1))])
param_grid = {'vect__stop_words': [None, stop_words], 'vect__max_features': [5000, 10000, 15000, None], 'vect__max_df':[0.5, 0.7, 1.0], 
              'lr__C':[0.1, 1, 10], }
gs_lr = GridSearchCV(pipeline_lr, param_grid, n_jobs=-1, cv=10, verbose=10)
gs_lr.fit(body, y)

print(gs_lr.best_params_)
print(gs_lr.best_score_)
gs_lr_best = gs_lr.best_estimator_

"""### 2-grams"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 2))), ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, solver='saga', n_jobs=-1))])
param_grid = {'vect__max_features': [50000, 60000, 70000, None],'vect__max_df':[0.5, 0.7, 1.0], 
              'lr__C':[10, 20]}
gs_lr = GridSearchCV(pipeline_lr, param_grid, n_jobs=-1, cv=10, verbose=10)
gs_lr.fit(body, y)

print(gs_lr.best_params_)
print(gs_lr.best_score_)
gs_lr_best = gs_lr.best_estimator_

pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, max_features=60000, max_df=0.5, ngram_range=(1, 2))),
                        ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, C=20, solver='saga', n_jobs=-1))])
scores = cross_val_score(pipeline_lr, body, y, cv=10)
scores.mean()

"""### 3-grams"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 3))), ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, solver='saga', n_jobs=-1))])
param_grid = {'vect__max_features': [80000, 90000, 100000, None],'vect__max_df':[0.5, 0.7, 1.0], 'vect__min_df':[1, 5, 10],
              'lr__C':[10, 30]}
gs_lr = GridSearchCV(pipeline_lr, param_grid, n_jobs=-1, cv=10, verbose=10)
gs_lr.fit(body, y)

print(gs_lr.best_params_)
print(gs_lr.best_score_)
gs_lr_best = gs_lr.best_estimator_

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words,max_df = 0.7, max_features=80000, ngram_range=(1, 3))), ('norm', Normalizer()),
                        ('lr', LogisticRegression(max_iter=10000, C=30, solver='saga', n_jobs=-1))])
scores = cross_val_score(pipeline_lr, body, y, cv=10)
scores.mean()

"""## Lemmatization"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")
# Lemmatization
body = train_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
body_test = test_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))

"""### 1-gram"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer()), ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, n_jobs=-1))])
param_grid = {'vect__stop_words': [None, stop_words], 'vect__max_features': [5000, 10000, 15000, None],
              'lr__C':[0.1, 1, 10], }
gs_lr = GridSearchCV(pipeline_lr, param_grid, n_jobs=-1, cv=10, verbose=10)
gs_lr.fit(body, y)

print(gs_lr.best_params_)
print(gs_lr.best_score_)
gs_lr_best = gs_lr.best_estimator_

"""### 2-grams"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 2))), ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, solver='saga', n_jobs=-1))])
param_grid = {'vect__max_features': [50000, 60000, 70000, None],'vect__max_df':[0.5, 0.7, 1.0], 'vect__min_df':[1, 10, 50, 100],
              'lr__C':[10, 20]}
gs_lr = GridSearchCV(pipeline_lr, param_grid, n_jobs=-1, cv=10, verbose=10)
gs_lr.fit(body, y)

print(gs_lr.best_params_)
print(gs_lr.best_score_)
gs_lr_best = gs_lr.best_estimator_

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, max_features=70000, max_df=0.45, ngram_range=(1, 2))),
                        ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, C=20, solver='saga', n_jobs=-1))])
scores = cross_val_score(pipeline_lr, body, y, cv=10)
scores.mean()

"""### 3-grams"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 3))), ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, solver='saga', n_jobs=-1))])
param_grid = {'vect__max_features': [80000, 90000, 100000, None],'vect__max_df':[0.5, 0.7, 1.0], 'vect__min_df':[1, 5, 10],
              'lr__C':[10, 30]}
gs_lr = GridSearchCV(pipeline_lr, param_grid, n_jobs=-1, cv=10, verbose=10)
gs_lr.fit(body, y)

print(gs_lr.best_params_)
print(gs_lr.best_score_)
gs_lr_best = gs_lr.best_estimator_

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words,max_df = 0.7, max_features=90000, ngram_range=(1, 3))), ('norm', Normalizer()),
                        ('lr', LogisticRegression(max_iter=10000, C=30, solver='saga', n_jobs=-1))])
scores = cross_val_score(pipeline_lr, body, y, cv=10)
scores.mean()

"""## Best Model
Lemmatization, 2-grams, TFIDF with stop words, max_features = 70000, max_df = 0.5, min_df = 1, Logistic Regression with C = 20
"""

start = time.time()
stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, max_features=70000, max_df=0.5, ngram_range=(1, 2))),
                        ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, C=20, solver='saga', n_jobs=-1))])
scores = cross_val_score(pipeline_lr, body, y, cv=10)
end = time.time()
print("Cross validation score: {}".format(scores.mean()))
print("Running time for Logistic Regression: {}".format(end - start))

start = time.time()
stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words, max_features=70000, max_df=0.5, ngram_range=(1, 2))),
                        ('norm', Normalizer()), ('lr', LogisticRegression(max_iter=10000, C=20, solver='saga', n_jobs=-1))])
pipeline_lr.fit(body, y)
end = time.time()
print("Running time for Logistic Regression: {}".format(end - start))

"""#Support Vector Machine

##Basic Case - no stem or lemmatization
"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")

"""### 1-gram"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_svc = Pipeline([('vect', TfidfVectorizer()), ('norm', Normalizer()), ('svc', svm.SVC())])
param_grid = {'vect__stop_words' : [stop_words, None], 'vect__max_features' : [5000, None],
              'svc__kernel' : ['linear', 'poly', 'rbf', 'sigmoid']}

gs_svc = GridSearchCV(pipeline_svc, param_grid, n_jobs = -1, cv = 5, verbose = 10)
gs_svc.fit(train_set.body, train_set.subreddit)

print(gs_svc.cv_results_)
print(gs_svc.best_params_)
print(gs_svc.best_score_)

"""### 2-gram"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_svc = Pipeline([('vect', TfidfVectorizer(ngram_range = (1, 2))), ('norm', Normalizer()), ('svc', svm.SVC())])
param_grid = {'vect__stop_words' : [stop_words, None], 'vect__max_features' : [70000, None],
              'svc__kernel' : ['linear', 'poly', 'rbf', 'sigmoid']}

gs_svc = GridSearchCV(pipeline_svc, param_grid, n_jobs = -1, cv = 5, verbose = 10)
gs_svc.fit(train_set.body, train_set.subreddit)

print(gs_svc.cv_results_)
print(gs_svc.best_params_)
print(gs_svc.best_score_)

"""## Stem"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")

body_stem = train_set.body.apply(tokenize).apply(stem).str.join(' ')
body_test_stem = test_set.body.apply(tokenize).apply(stem).str.join(' ')

"""### 1-gram"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_svc = Pipeline([('vect', TfidfVectorizer()), ('norm', Normalizer()), ('svc', svm.SVC())])
param_grid = {'vect__stop_words' : [stop_words, None], 'vect__max_features' : [5000, None],
              'svc__kernel' : ['linear', 'poly', 'rbf', 'sigmoid']}
gs_svc_stem = GridSearchCV(pipeline_svc, param_grid, n_jobs = -1, cv = 5, verbose = 10)
gs_svc_stem.fit(body_stem, train_set.subreddit)

print(gs_svc_stem.cv_results_)
print(gs_svc_stem.best_params_)
print(gs_svc_stem.best_score_)

"""### 2-gram"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_svc = Pipeline([('vect', TfidfVectorizer(ngram_range = (1, 2))), ('norm', Normalizer()), ('svc', svm.SVC())])
param_grid = {'vect__stop_words' : [stop_words, None], 'vect__max_features' : [70000, None],
              'svc__kernel' : ['linear', 'poly', 'rbf', 'sigmoid']}

gs_svc_stem = GridSearchCV(pipeline_svc, param_grid, n_jobs = -1, cv = 5, verbose = 10)
gs_svc_stem.fit(body_stem, train_set.subreddit)

print(gs_svc_stem.cv_results_)
print(gs_svc_stem.best_params_)
print(gs_svc_stem.best_score_)

"""## Lemmatization"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")

body_lemma = train_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
body_test_lemma = test_set.body.apply(tokenize).apply(lemmatization).str.join(' ')

"""### 1-gram"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_svc = Pipeline([('vect', TfidfVectorizer()), ('norm', Normalizer()), ('svc', svm.SVC())])
param_grid = {'vect__stop_words' : [stop_words, None], 'vect__max_features' : [5000, None],
              'svc__kernel' : ['linear', 'poly', 'rbf', 'sigmoid']}
gs_svc_lemma = GridSearchCV(pipeline_svc, param_grid, n_jobs = -1, cv = 5, verbose = 10)
gs_svc_lemma.fit(body_lemma, train_set.subreddit)

print(gs_svc_lemma.cv_results_)
print(gs_svc_lemma.best_params_)
print(gs_svc_lemma.best_score_)

"""### 2-gram"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_svc = Pipeline([('vect', TfidfVectorizer(ngram_range = (1, 2))), ('norm', Normalizer()), ('svc', svm.SVC())])
param_grid = {'vect__stop_words' : [stop_words, None], 'vect__max_features' : [70000, None],
              'svc__kernel' : ['linear', 'poly', 'rbf', 'sigmoid']}

gs_svc_lemma = GridSearchCV(pipeline_svc, param_grid, n_jobs = -1, cv = 5, verbose = 10)
gs_svc_lemma.fit(body_lemma, train_set.subreddit)

print(gs_svc_lemma.cv_results_)
print(gs_svc_lemma.best_params_)
print(gs_svc_lemma.best_score_)

"""## Best Model"""

train_set = pd.read_csv("/content/drive/My Drive/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/ECSE 551 Group 3/Assignment 2/test.csv")

body_lemma = train_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
body_test_lemma = test_set.body.apply(tokenize).apply(lemmatization).str.join(' ')

#best pipeline - no max features 
best_pipe = Pipeline([('vect', TfidfVectorizer(max_features = None, stop_words = text.ENGLISH_STOP_WORDS)), ('norm', Normalizer()), ('svc', svm.SVC(kernel = 'linear'))])

# cv - no max features

kf = KFold(n_splits = 5, shuffle = True)
avgAccuracy = 0
for train, test in kf.split(body_lemma):
  print("Train: ", train)
  print("Test: ", test)
  best_pipe.fit(body_lemma[train], train_set.subreddit[train])
  preds = best_pipe.predict(body_lemma[test])
  accuracy = np.sum(np.where(preds == train_set.subreddit[test], 1, 0)) / train_set.subreddit[test].size
  print(accuracy)
  avgAccuracy += accuracy

print("Avg accuracy = " + str(avgAccuracy / 5))

#best pipeline - 70000 features
best_pipe = Pipeline([('vect', TfidfVectorizer(max_features = 70000, stop_words = text.ENGLISH_STOP_WORDS)), ('norm', Normalizer()), ('svc', svm.SVC(kernel = 'linear'))])
best_pipe.fit(body_lemma, train_set.subreddit)
preds = best_pipe.predict(body_test_lemma)

# cv - 70000 features
kf = KFold(n_splits = 5, shuffle = True)
avgAccuracy = 0
for train, test in kf.split(body_lemma):
  print("Train: ", train)
  print("Test: ", test)
  # print(body_lemma.to_numpy()[train])
  # print(train_set.subreddit.to_numpy()[train])
  best_pipe.fit(body_lemma.to_numpy()[train], train_set.subreddit.to_numpy()[train])
  preds = best_pipe.predict(body_lemma.to_numpy()[test])
  accuracy = np.sum(preds == train_set.subreddit.to_numpy()[test]) / train_set.subreddit.to_numpy()[test].shape[0]
  print(accuracy)
  avgAccuracy += accuracy

print("Avg accuracy = " + str(avgAccuracy / 5))

"""# Linear Discriminant Analysis"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")

body = train_set.body
body_stem = train_set.body.apply(tokenize).apply(stem).str.join(' ')
body_lemma = train_set.body.apply(tokenize).apply(lemmatization).str.join(' ')

encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))

"""## Grid Search

### Pre-processing
"""

#Split the training set for grid search
Xtrain = body.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(Xtrain, y, train_size=0.8, test_size=0.2,random_state = 0)

stop_words = text.ENGLISH_STOP_WORDS
normalizer_train = Normalizer()
#basic countvectorizer
vectorizer = CountVectorizer(max_features = 5000)

vectors_train = vectorizer.fit_transform(X_train)
vectors_test = vectorizer.transform(X_test)

vectors_train= normalizer_train.transform(vectors_train)
vectors_test = normalizer_train.transform(vectors_test)

#basic countvectorizer with stop_words
vectorizer = CountVectorizer(stop_words = stop_words, max_features = 5000)

vectors_train_stop = vectorizer.fit_transform(X_train)
vectors_test_stop = vectorizer.transform(X_test)

vectors_train_stop= normalizer_train.transform(vectors_train_stop)
vectors_test_stop = normalizer_train.transform(vectors_test_stop)

#td-idf countvectorizer
tf_idf_vectorizer = TfidfVectorizer(stop_words = stop_words, max_features = 5000)

vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)
vectors_test_idf = tf_idf_vectorizer.transform(X_test)

vectors_train_idf= normalizer_train.transform(vectors_train_idf)
vectors_test_idf = normalizer_train.transform(vectors_test_idf)

#stem
Xtrain = body_stem.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(Xtrain, y, train_size=0.8, test_size=0.2,random_state = 0)

vectorizer = CountVectorizer(stop_words = stop_words,max_features = 5000)
vectors_train_stem = vectorizer.fit_transform(X_train)
vectors_test_stem = vectorizer.transform(X_test)

vectors_train_stem= normalizer_train.transform(vectors_train_stem)
vectors_test_stem = normalizer_train.transform(vectors_test_stem)

#lemmatization
Xtrain = body_lemma.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(Xtrain, y, train_size=0.8, test_size=0.2,random_state = 0)

vectorizer = CountVectorizer(stop_words = stop_words,max_features = 5000)
vectors_train_Lemma = vectorizer.fit_transform(X_train)
vectors_test_Lemma = vectorizer.transform(X_test)
vectors_train_Lemma= normalizer_train.transform(vectors_train_Lemma)
vectors_test_Lemma = normalizer_train.transform(vectors_test_Lemma)

estimator_lda = LinearDiscriminantAnalysis()
parameters_lda = {
    'solver': ('lsqr','eigen','svd'), 
               }
# with GridSearch
grid_search_lda = GridSearchCV(
    estimator=estimator_lda ,
    param_grid=parameters_lda,
    scoring = 'accuracy',
    n_jobs = -1,
    verbose = 10,
    cv = 5
)

lda_simple=grid_search_lda.fit(vectors_train.toarray(), y_train)
y_pred_simple =lda_simple.predict(vectors_test.toarray())

lda_stop=grid_search_lda.fit(vectors_train_stop.toarray(), y_train)
y_pred_stop =lda_stop.predict(vectors_test_stop.toarray())

lda_idf=grid_search_lda.fit(vectors_train_idf.toarray(), y_train)
y_pred_idf =lda_idf.predict(vectors_test_idf.toarray())

lda_stem=grid_search_lda.fit(vectors_train_stem.toarray(), y_train)
y_pred_stem =lda_stem.predict(vectors_test_stem.toarray())

lda_lemma=grid_search_lda.fit(vectors_train_Lemma.toarray(), y_train)
y_pred_lemma =lda_lemma.predict(vectors_test_Lemma.toarray())

print(lda_simple.best_params_)
print(lda_stop.best_params_)
print(lda_idf.best_params_)
print(lda_stem.best_params_)
print(lda_lemma.best_params_)

print ('All max features are set to 5000. All are simple vectorizer except the TD-IDF one.')
print('Accuracy Score - LDA - Simple:', metrics.accuracy_score(y_test, y_pred_simple))  
print('Accuracy Score - LDA - Stop words:', metrics.accuracy_score(y_test, y_pred_stop)) 
print('Accuracy Score - LDA - TD-IDF:', metrics.accuracy_score(y_test, y_pred_idf)) 
print('Accuracy Score - LDA - Stem:', metrics.accuracy_score(y_test, y_pred_stem)) 
print('Accuracy Score - LDA - Lemmatized:', metrics.accuracy_score(y_test, y_pred_lemma))

"""## Basic Case - no stem or lemmatization"""

#Run this first
normalizer_train = Normalizer()
stop_words = text.ENGLISH_STOP_WORDS
Xtrain = body.to_numpy()

"""### 1-gram"""

#tdidf with stop_words, 1-gram, 1000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 1000)
vectors_train1a = vectorizer.fit_transform(Xtrain)
vectors_train1a = normalizer_train.transform(vectors_train1a)

#tdidf with stop_words, 1-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500)
vectors_train1b = vectorizer.fit_transform(Xtrain)
vectors_train1b = normalizer_train.transform(vectors_train1b)


#tdidf with stop_words, 1-gram, 5000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 5000)
vectors_train1c = vectorizer.fit_transform(Xtrain)
vectors_train1c = normalizer_train.transform(vectors_train1c)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
scores1a = cross_val_score(lda,vectors_train1a.toarray(), y, cv=10)
scores1b = cross_val_score(lda,vectors_train1b.toarray(), y, cv=10)
scores1c = cross_val_score(lda,vectors_train1c.toarray(), y, cv=10)
print('CV- Basic - 1000:',scores1a.mean())
print('CV- Basic - 2500:',scores1b.mean())
print('CV- Basic - 5000:',scores1c.mean())

"""### 2-gram"""

#tdidf with stop_words, 2-gram, 1000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 1000,ngram_range = (1, 2))
vectors_train1a = vectorizer.fit_transform(Xtrain)
vectors_train1a = normalizer_train.transform(vectors_train1a)

#tdidf with stop_words, 2-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500,ngram_range = (1, 2))
vectors_train1b = vectorizer.fit_transform(Xtrain)
vectors_train1b = normalizer_train.transform(vectors_train1b)


#tdidf with stop_words, 2-gram, 5000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 5000,ngram_range = (1, 2))
vectors_train1c = vectorizer.fit_transform(Xtrain)
vectors_train1c = normalizer_train.transform(vectors_train1c)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
#scores1a = cross_val_score(lda,vectors_train1a.toarray(), y, cv=5)
scores1b = cross_val_score(lda,vectors_train1b.toarray(), y, cv=5)
#scores1c = cross_val_score(lda,vectors_train1c.toarray(), y, cv=5)
#print('CV- Basic - 2gram - 1000:',scores1a.mean())
print('CV- Basic - 2gram - 2500:',scores1b.mean())
#print('CV- Basic - 2gram - 5000:',scores1c.mean())

"""### 3-gram"""

#tdidf with stop_words, 3-gram, 1000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 1000,ngram_range = (1, 3))
vectors_train1a = vectorizer.fit_transform(Xtrain)
vectors_train1a = normalizer_train.transform(vectors_train1a)

#tdidf with stop_words, 3-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500,ngram_range = (1, 3))
vectors_train1b = vectorizer.fit_transform(Xtrain)
vectors_train1b = normalizer_train.transform(vectors_train1b)


#tdidf with stop_words, 3-gram, 5000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 5000,ngram_range = (1, 3))
vectors_train1c = vectorizer.fit_transform(Xtrain)
vectors_train1c = normalizer_train.transform(vectors_train1c)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
#scores1a = cross_val_score(lda,vectors_train1a.toarray(), y, cv=5)
scores1b = cross_val_score(lda,vectors_train1b.toarray(), y, cv=5)
#scores1c = cross_val_score(lda,vectors_train1c.toarray(), y, cv=5)
#print('CV- Basic - 3gram - 1000:',scores1a.mean())
print('CV- Basic - 3gram - 2500:',scores1b.mean())
#print('CV- Basic - 3gram - 5000:',scores1c.mean())

"""## Stem"""

#Run this before running the following subsections
normalizer_train = Normalizer()
stop_words = text.ENGLISH_STOP_WORDS
Xtrain = body_stem.to_numpy()

"""### 1-gram"""

#tdidf with stop_words, 1-gram, 1000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 1000)
vectors_train_stem1a = vectorizer.fit_transform(Xtrain)
vectors_train_stem1a= normalizer_train.transform(vectors_train_stem1a)

#tdidf with stop_words, 1-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500)
vectors_train_stem1b = vectorizer.fit_transform(Xtrain)
vectors_train_stem1b= normalizer_train.transform(vectors_train_stem1b)


#tdidf with stop_words, 1-gram, 5000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 5000)
vectors_train_stem1c = vectorizer.fit_transform(Xtrain)
vectors_train_stem1c= normalizer_train.transform(vectors_train_stem1c)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
#scores1a = cross_val_score(lda,vectors_train_stem1a.toarray(), y, cv=5)
scores1b = cross_val_score(lda,vectors_train_stem1b.toarray(), y, cv=5)
#scores1c = cross_val_score(lda,vectors_train_stem1c.toarray(), y, cv=5)
#print('CV- Stem - 1gram - 1000:',scores1a.mean())
print('CV- Stem - 1gram - 2500:',scores1b.mean())
#print('CV- Stem - 1gram - 5000:',scores1c.mean())

"""### 2-gram"""

#tdidf with stop_words, 2-gram, 1000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 1000,ngram_range = (1, 2))
vectors_train_stem1a = vectorizer.fit_transform(Xtrain)
vectors_train_stem1a = normalizer_train.transform(vectors_train_stem1a)

#tdidf with stop_words, 2-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500,ngram_range = (1, 2))
vectors_train_stem1b = vectorizer.fit_transform(Xtrain)
vectors_train_stem1b = normalizer_train.transform(vectors_train_stem1b)


#tdidf with stop_words, 2-gram, 5000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 5000,ngram_range = (1, 2))
vectors_train_stem1c = vectorizer.fit_transform(Xtrain)
vectors_train_stem1c = normalizer_train.transform(vectors_train_stem1c)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
#scores1a = cross_val_score(lda,vectors_train_stem1a.toarray(), y, cv=5)
scores1b = cross_val_score(lda,vectors_train_stem1b.toarray(), y, cv=5)
#scores1c = cross_val_score(lda,vectors_train_stem1c.toarray(), y, cv=5)
#print('CV- Stem - 2gram - 1000:',scores1a.mean())
print('CV- Stem - 2gram - 2500:',scores1b.mean())
#print('CV- Stem - 2gram - 5000:',scores1c.mean())

"""### 3-gram"""

#tdidf with stop_words, 2-gram, 1000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 1000,ngram_range = (1, 3))
vectors_train_stem1a = vectorizer.fit_transform(Xtrain)
vectors_train_stem1a = normalizer_train.transform(vectors_train_stem1a)

#tdidf with stop_words, 2-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500,ngram_range = (1, 3))
vectors_train_stem1b = vectorizer.fit_transform(Xtrain)
vectors_train_stem1b = normalizer_train.transform(vectors_train_stem1b)

#tdidf with stop_words, 2-gram, 5000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 5000,ngram_range = (1, 3))
vectors_train_stem1c = vectorizer.fit_transform(Xtrain)
vectors_train_stem1c = normalizer_train.transform(vectors_train_stem1c)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
#scores1a = cross_val_score(lda,vectors_train_stem1a.toarray(), y, cv=5)
scores1b = cross_val_score(lda,vectors_train_stem1b.toarray(), y, cv=5)
#scores1c = cross_val_score(lda,vectors_train_stem1c.toarray(), y, cv=5)
#print('CV- Stem - 3gram - 1000:',scores1a.mean())
print('CV- Stem - 3gram - 2500:',scores1b.mean())
#print('CV- Stem - 3gram - 5000:',scores1c.mean())

"""## Lemmatization"""

#Run this cell before all the other cells
normalizer_train = Normalizer()
stop_words = text.ENGLISH_STOP_WORDS
Xtrain = body_lemma.to_numpy()

"""### 1-gram"""

#tdidf with stop_words, 1-gram, 1000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 1000,ngram_range = (1, 1))
vectors_train_lemma1a = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1a = normalizer_train.transform(vectors_train_lemma1a)

#tdidf with stop_words, 1-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500,ngram_range = (1, 1))
vectors_train_lemma1b = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1b = normalizer_train.transform(vectors_train_lemma1b)


#tdidf with stop_words, 1-gram, 5000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 5000,ngram_range = (1, 1))
vectors_train_lemma1c = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1c = normalizer_train.transform(vectors_train_lemma1c)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
#scores1a = cross_val_score(lda,vectors_train_lemma1a.toarray(), y, cv=5)
scores1b = cross_val_score(lda,vectors_train_lemma1b.toarray(), y, cv=5)
#scores1c = cross_val_score(lda,vectors_train_lemma1c.toarray(), y, cv=5)
#print('CV- Lemma - 3gram - 1000:',scores1a.mean())
print('CV- Lemma - 3gram - 2500:',scores1b.mean())
#print('CV- Lemma - 3gram - 5000:',scores1c.mean())

"""### 2-gram"""

normalizer_train = Normalizer()
stop_words = text.ENGLISH_STOP_WORDS
Xtrain = body_lemma.to_numpy()

#tdidf with stop_words, 2-gram, 1000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 1000,ngram_range = (1, 2))
vectors_train_lemma1a = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1a = normalizer_train.transform(vectors_train_lemma1a)

#tdidf with stop_words, 2-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500,ngram_range = (1, 2))
vectors_train_lemma1b = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1b = normalizer_train.transform(vectors_train_lemma1b)


#tdidf with stop_words, 2-gram, 5000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 5000,ngram_range = (1, 2))
vectors_train_lemma1c = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1c = normalizer_train.transform(vectors_train_lemma1c)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
#scores1a = cross_val_score(lda,vectors_train_lemma1a.toarray(), y, cv=5)
scores1b = cross_val_score(lda,vectors_train_lemma1b.toarray(), y, cv=5)
#scores1c = cross_val_score(lda,vectors_train_lemma1c.toarray(), y, cv=5)
#print('CV- Leamma - 3gram - 1000:',scores1a.mean())
print('CV- Lemma - 3gram - 2500:',scores1b.mean())
#print('CV- Lemma - 3gram - 5000:',scores1c.mean())

"""### 3-gram"""

normalizer_train = Normalizer()
stop_words = text.ENGLISH_STOP_WORDS
Xtrain = body_lemma.to_numpy()

#tdidf with stop_words, 3-gram, 1000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 1000,ngram_range = (1,3))
vectors_train_lemma1a = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1a = normalizer_train.transform(vectors_train_lemma1a)

#tdidf with stop_words, 3-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500,ngram_range = (1, 3))
vectors_train_lemma1b = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1b = normalizer_train.transform(vectors_train_lemma1b)


#tdidf with stop_words, 3-gram, 5000 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 5000,ngram_range = (1, 3))
vectors_train_lemma1c = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1c = normalizer_train.transform(vectors_train_lemma1c)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
#scores1a = cross_val_score(lda,vectors_train_lemma1a.toarray(), y, cv=5)
scores1b = cross_val_score(lda,vectors_train_lemma1b.toarray(), y, cv=5)
#scores1c = cross_val_score(lda,vectors_train_lemma1c.toarray(), y, cv=5)
#print('CV- Lemma - 3gram - 1000:',scores1a.mean())
print('CV- Lemma - 3gram - 2500:',scores1b.mean())
#print('CV- Lemma - 3gram - 5000:',scores1c.mean())

"""## Best Model

Lemmatization, 1-gram, 2500 max features, solver = lsqr
"""

#Run this cell before all the other cells
normalizer_train = Normalizer()
stop_words = text.ENGLISH_STOP_WORDS
Xtrain = body_lemma.to_numpy()

start = time.time()

#tdidf with stop_words, 1-gram, 2500 features
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500,ngram_range = (1, 1))
vectors_train_lemma1b = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1b = normalizer_train.transform(vectors_train_lemma1b)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
scores = cross_val_score(lda, vectors_train_lemma1b.toarray(), y, cv=10)

end = time.time()
print("Cross validation score: {}".format(scores.mean()))
print("Running time for LDA: {}".format(end - start))

start = time.time()
vectorizer = TfidfVectorizer(stop_words = stop_words,max_features = 2500,ngram_range = (1, 1))
vectors_train_lemma1b = vectorizer.fit_transform(Xtrain)
vectors_train_lemma1b = normalizer_train.transform(vectors_train_lemma1b)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')
lda.fit(vectors_train_lemma1b.toarray(), y)
end = time.time()
print("Running time for LDA: {}".format(end - start))

"""#Submission"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")
body = train_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
body_test = test_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))

"""## Naive Bayes"""

stop_words = text.ENGLISH_STOP_WORDS
encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))
body = train_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
body_test = test_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
vectorizer = CountVectorizer(max_features=10000, stop_words = stop_words, binary=True)
X = vectorizer.fit_transform(body).toarray()
model = NaiveBayes()
model.fit(X, y)
predict = model.predict(vectorizer.transform(body_test).toarray())
submission_nb = pd.DataFrame()
submission_nb['id'] = test_set.id
submission_nb['subreddit'] = predict
reverse_encoding = {k: v for (v, k) in encoding.items()}
submission_nb.subreddit = submission_nb.subreddit.apply(lambda x: reverse_encoding[x])
submission_nb.to_csv("/content/drive/My Drive/ECSE 551 Group 3/Assignment 2/team3_submission_my_Naive_Bayes_lemma.csv", index=False)

"""## Logistic Regression"""

stop_words = text.ENGLISH_STOP_WORDS
pipeline_lr = Pipeline([('vect', TfidfVectorizer(stop_words=stop_words,max_df = 0.7, max_features=90000, ngram_range=(1, 3))), ('norm', Normalizer()),
                        ('lr', LogisticRegression(max_iter=10000, C=30, solver='saga', n_jobs=-1))])

pipeline_lr.fit(body, y)
predict = pipeline_lr.predict(body_test)
submission_lr = pd.DataFrame()
submission_lr['id'] = test_set.id
submission_lr['subreddit'] = predict
reverse_encoding = {k: v for (v, k) in encoding.items()}
submission_lr.subreddit = submission_lr.subreddit.apply(lambda x: reverse_encoding[x])
submission_lr.to_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/team3_submission_lr_3_grams_lemma.csv", index=False)

"""## Support Vector Machine"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2//train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")

body_lemma = train_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
body_test_lemma = test_set.body.apply(tokenize).apply(lemmatization).str.join(' ')

# submission - no max features
stop_words = text.ENGLISH_STOP_WORDS
best_pipe = Pipeline([('vect', TfidfVectorizer(max_features = None, stop_words = text.ENGLISH_STOP_WORDS)), ('norm', Normalizer()), ('svc', svm.SVC(kernel = 'linear'))])
best_pipe.fit(body, train_set.subreddit)
preds = best_pipe.predict(body_test_lemma)

submission_svc = pd.DataFrame()
submission_svc['id'] = test_set.id
submission_svc['subreddit'] = preds
print(submission_svc)
submission_svc.to_csv("/content/drive/My Drive/ECSE 551 Group 3/Assignment 2/team3_SVC_lemma.csv", index=False)

# submission - 70000 features
stop_words = text.ENGLISH_STOP_WORDS
best_pipe = Pipeline([('vect', TfidfVectorizer(max_features = 70000, stop_words = text.ENGLISH_STOP_WORDS)), ('norm', Normalizer()), ('svc', svm.SVC(kernel = 'linear'))])
best_pipe.fit(body_lemma, train_set.subreddit)
preds = best_pipe.predict(body_test_lemma)

submission_svc = pd.DataFrame()
submission_svc['id'] = test_set.id
submission_svc['subreddit'] = preds
print(submission_svc)
submission_svc.to_csv("/content/drive/My Drive/ECSE 551 Group 3/Assignment 2/team3_SVC_lemma_2gram.csv", index=False)

"""## Linear Discriminant Analysis"""

train_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/train.csv")
test_set = pd.read_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/test.csv")
body = train_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
body_test = test_set.body.apply(tokenize).apply(lemmatization).str.join(' ')
encoding = {'rpg':0, 'anime':1, 'datascience':2, 'hardware':3, 'cars':4, 'gamernews':5, 'gamedev':6, 'computers':7}
y = np.array(train_set.subreddit.apply(lambda x: encoding[x]))

stop_words = text.ENGLISH_STOP_WORDS
normalizer_train = Normalizer()

body_array = body.to_numpy()
body_test_array = body_test.to_numpy()

vectorizer = TfidfVectorizer(stop_words = stop_words,  ngram_range=(1, 1),max_features = 2500)
vectors_train = vectorizer.fit_transform(body_array)
vectors_test = vectorizer.transform(body_test_array)
vectors_train= normalizer_train.transform(vectors_train)
vectors_test = normalizer_train.transform(vectors_test)

lda = LinearDiscriminantAnalysis(solver = 'lsqr')

lda.fit(vectors_train.toarray(), y)
predict = lda.predict(vectors_test.toarray())
submission_lda = pd.DataFrame()
submission_lda['id'] = test_set.id
submission_lda['subreddit'] = predict
reverse_encoding = {k: v for (v, k) in encoding.items()}
submission_lda.subreddit = submission_lda.subreddit.apply(lambda x: reverse_encoding[x])
submission_lda.to_csv("/content/drive/My Drive/PhD /Courses/ECSE 551/ECSE 551 Group 3/Assignment 2/team3_submission_lda_1_grams_lemma.csv", index=False)